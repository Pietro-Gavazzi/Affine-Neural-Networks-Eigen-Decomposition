{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e021fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This code was made with the help of chat-gpt\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv, pinv, norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5875df8b",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6086ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def generate_linear_weight(input_size, hidden_layer_size_list, output_size, epsilon):\n",
    "    \"\"\"\n",
    "    The aim of this code is to generate the weight matrix so that the weights follows the condition for epsilon\n",
    "    THIS HAS NOT BEEN ACHIEVED ! the epsilon in this code is not the thoretical epsilon\n",
    "    \"\"\"\n",
    "    nb_elements = 0\n",
    "    previous_size = 0\n",
    "    for next_size in hidden_layer_size_list:\n",
    "        nb_elements += previous_size*next_size\n",
    "        previous_size = next_size\n",
    "    nb_elements += previous_size*output_size\n",
    "\n",
    "    prob = 0.01\n",
    "\n",
    "    sigma = np.sqrt ((epsilon*prob)/(7*nb_elements))\n",
    "    # print(f\"sigma is {sigma}\")\n",
    "\n",
    "    weight_list = []\n",
    "    previous_size = input_size\n",
    "\n",
    "    zeros= 0\n",
    "    for next_size in hidden_layer_size_list:\n",
    "        weight_list.append(np.random.random(size = (previous_size, next_size))*sigma*zeros)\n",
    "        previous_size = next_size\n",
    "        zeros = (zeros or 1)\n",
    "        # print(f\"zeros is {zeros}\")\n",
    "    \n",
    "    weight_list.append((np.random.random(size = (previous_size, next_size))*sigma).T[0].reshape((previous_size, 1)))\n",
    "\n",
    "    return weight_list\n",
    "\n",
    "\n",
    "def generate_data(n, d, noise_variance = .1, bias=1):\n",
    "    \"\"\"Generates random Gaussian data with noise.\"\"\"\n",
    "    X = np.random.normal(0, 1, (n, d))\n",
    "    Xtest = np.random.normal(0, 1, (n, d))\n",
    "    w_true = np.random.normal(0, 1, (d,1))\n",
    "    noise = np.random.normal(0, 1, (n, 1))\n",
    "    y = X @ w_true + noise_variance * noise + bias\n",
    "    noise2 = np.random.normal(0, 1, (n,1))\n",
    "    ytest = Xtest @ w_true + noise_variance * noise2 + bias\n",
    "    w_star = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "    return (X, y, Xtest, ytest, w_star, w_true)\n",
    "\n",
    "\n",
    "def make_augmented_matrix(slope, bias):\n",
    "    in_dim, out_dim = slope.shape\n",
    "\n",
    "    # Create augmented matrix of shape (in_dim+1, out_dim+1)\n",
    "    aug = np.zeros((in_dim + 1, out_dim + 1))\n",
    "\n",
    "    # Top-left corner: 1\n",
    "    aug[0, 0] = 1\n",
    "\n",
    "    # Top row (bias)\n",
    "    aug[0, 1:] = bias\n",
    "\n",
    "    # Bottom-left: already 0\n",
    "    # Bottom-right: slope\n",
    "    aug[1:, 1:] = slope\n",
    "\n",
    "    return aug\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf4c9fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AffineModel():\n",
    "    def __init__(self, input_size, hidden_layer_size_list, output_size, epsilon=1):\n",
    "        self.input_size= input_size\n",
    "\n",
    "        \n",
    "        hidden_layer_size_list_affine = np.copy(hidden_layer_size_list)+1\n",
    "\n",
    "        # Initialize the weights\n",
    "        weight_list_affine = generate_linear_weight(input_size+1,hidden_layer_size_list_affine, output_size+1, epsilon) \n",
    "        self.biases = []\n",
    "        self.slopes = []\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(len(weight_list_affine)-1):\n",
    "            # print(i)\n",
    "            # print(weight_list_affine[i].shape)\n",
    "            bias = weight_list_affine[i][0, 1:]\n",
    "            # print(bias.shape)\n",
    "            self.biases.append(bias)\n",
    "            slope = weight_list_affine[i][1:, 1:]\n",
    "            # print(slope.shape)\n",
    "            self.slopes.append(slope)\n",
    "            # print()\n",
    "        \n",
    "        # last slope and bias\n",
    "\n",
    "        i+=1 \n",
    "        # print(i)\n",
    "        # print(weight_list_affine[i])\n",
    "        bias = weight_list_affine[i][0, 0:]\n",
    "        # print(bias.shape)\n",
    "        self.biases.append(bias)\n",
    "        slope = weight_list_affine[i][1:, 0:]\n",
    "        # print(slope.shape)\n",
    "        self.slopes.append(slope)\n",
    "        # print()\n",
    "\n",
    "        # print(self.biases)\n",
    "        # print(self.slopes)\n",
    "\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "\n",
    "\n",
    "    def step(self, X_train, y_train, lr=0.01):\n",
    "        # Forward pass\n",
    "        y_pred = self.forward(X_train)\n",
    "        N = X_train.shape[0]\n",
    "\n",
    "        # Compute gradient of loss w.r.t. output\n",
    "        dA = (2 / N) * (y_pred - y_train)   # dL/dA_last\n",
    "\n",
    "        # List to store gradients for each weight matrix\n",
    "        bias_grad_list = [None] * len(self.biases)\n",
    "        slopes_grad_list = [None] * len(self.slopes)\n",
    "\n",
    "        # Backward pass: reverse order\n",
    "        for i in reversed(range(len(bias_grad_list))):\n",
    "\n",
    "            W = self.slopes[i]\n",
    "            b = self.biases[i]\n",
    "\n",
    "            # to do: update\n",
    "\n",
    "            # a_{i-1} is either input X or previous partial output\n",
    "            if i == 0:\n",
    "                A_prev = self.lastX\n",
    "            else:\n",
    "                A_prev = self.partial[i-1]\n",
    "\n",
    "            # Gradient wrt W_i:   dW = A_prev.T @ dA\n",
    "            grad_W = A_prev.T @ dA\n",
    "            slopes_grad_list[i] = grad_W\n",
    "            grad_b = dA.sum(axis = 0)\n",
    "            bias_grad_list[i] = grad_b\n",
    "\n",
    "            # Backprop to previous layer: dA = dA @ W.T\n",
    "            dA = dA @ W.T\n",
    "\n",
    "        # Update parameters\n",
    "        for i in range(len(self.biases)):\n",
    "            self.biases[i] -= lr * bias_grad_list[i]\n",
    "            self.slopes[i] -= lr * slopes_grad_list[i]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "\n",
    "        if X.shape[1] != self.input_size:\n",
    "            print(\"size of input do not match size of model\")\n",
    "            return\n",
    "        \n",
    "        self.lastX = X\n",
    "        y = X\n",
    "\n",
    "        self.partial = []\n",
    "\n",
    "        for b, W in zip(self.biases, self.slopes):\n",
    "            y = b + y@W\n",
    "            self.partial.append(y)\n",
    "        return y\n",
    "    \n",
    "\n",
    "    def get_w_prod_and_bias(self):\n",
    "        \"\"\"\n",
    "        Computes the equivalent single-layer slope vector (W_prod)\n",
    "        and bias scalar/vector (b_prod) for the whole affine network.\n",
    "        \"\"\"\n",
    "        # Start with first layer\n",
    "        W_prod = self.slopes[0].copy()\n",
    "        b_prod = self.biases[0].copy()\n",
    "\n",
    "        for i in range(1, len(self.slopes)):\n",
    "            # Update the bias: propagate previous bias through next slope\n",
    "            b_prod = b_prod @ self.slopes[i] + self.biases[i]\n",
    "            # Update the slope: multiply slopes\n",
    "            W_prod = W_prod @ self.slopes[i]\n",
    "\n",
    "        return W_prod, b_prod\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8c1dc",
   "metadata": {},
   "source": [
    "# Test Affine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e1d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "np.random.seed(42)\n",
    "\n",
    "# We generate 6 data points and \n",
    "# We have 4 slope parameter + 1 bias parameter = 5 parameters in total\n",
    "# This means we are in the overdetermined setting: more equations than uknowns \n",
    "\n",
    "n = 6\n",
    "d = 4\n",
    "\n",
    "\n",
    "(X, y, Xtest, ytest, w_star, w_true) =  generate_data(n, d, noise_variance=10)\n",
    "\n",
    "\n",
    "# We define a model with 3 hidden layers\n",
    "model = AffineModel(d, [ d, d, d], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0435b763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The training error made by our affine model after training is: 46.796558334866575\n",
      "The error made by the the best possible model is:              46.796558334866575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "train_error = []\n",
    "train_error.append(np.mean((y-model(X))**2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "left_sing_vec_list = [[] for i in model.biases]\n",
    "eigs_list =  [[] for i in model.biases]\n",
    "right_sing_vec_list = [[] for i in model.biases]\n",
    "\n",
    "\n",
    "lemma_1_list = [[] for i in range(len (model.biases)-1)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nb_steps = 500000\n",
    "for i in range(nb_steps):\n",
    "\n",
    "    model.step(X, y, .001)\n",
    "\n",
    "\n",
    "    if i%(nb_steps/100) == 0:\n",
    "\n",
    "        train_error.append(np.mean((y-model(X))**2))\n",
    "\n",
    "\n",
    "        # print(np.log(train_error[-1]))\n",
    "\n",
    "        for  i in range(len(model.biases)) :\n",
    "\n",
    "            slope = model.slopes[i]\n",
    "            bias = model.biases[i]\n",
    "\n",
    "            weight = make_augmented_matrix(slope, bias)\n",
    "\n",
    "\n",
    "            if i != 0:\n",
    "                lemma_1_list[i-1].append(weight@weight.T - WktWk)\n",
    "\n",
    "\n",
    "            WktWk =  weight.T@weight\n",
    "\n",
    "            svd_decomp = np.linalg.svd(weight)\n",
    "\n",
    "            left_sing_vec = svd_decomp[0].T[0]\n",
    "            eig = svd_decomp[1][0]\n",
    "            right_sing_vec = svd_decomp[2][0]\n",
    "\n",
    "            left_sing_vec_list[i].append(left_sing_vec)\n",
    "            eigs_list[i].append(eig)\n",
    "            right_sing_vec_list[i].append(right_sing_vec)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print ()\n",
    "# print(train_error[0])\n",
    "print(f\"The training error made by our affine model after training is: {train_error[-1]}\")\n",
    "\n",
    "\n",
    "X_with_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "theta_star = np.linalg.inv(X_with_bias.T@X_with_bias)@X_with_bias.T@y\n",
    "OLS_train_error = np.mean((X_with_bias@theta_star-y)**2)\n",
    "\n",
    "\n",
    "print(f\"The error made by the the best possible model is:              {OLS_train_error}\")\n",
    "print ()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7f51f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix 1\n",
      "the norm of the bias vector is 0.21342896612233658\n",
      "value of the bias vector: [-0.14717603 -0.06195894 -0.09350177 -0.10634683]\n",
      "highest singular vaule: 1.6530041781081215\n",
      "second singular vaule: 0.9867655405235685\n",
      "sum of the rest of the singular values : 1.4757441689111546e-06\n",
      "\n",
      "matrix 2\n",
      "the norm of the bias vector is 0.15969017360820453\n",
      "value of the bias vector: [-0.05049915 -0.07429144 -0.09345881 -0.09325782]\n",
      "     > cosine similarity with previous right singular vector: -0.999576360815782\n",
      "highest singular vaule: 1.6561000754340094\n",
      "second singular vaule: 0.992661091962498\n",
      "sum of the rest of the singular values : 0.007270260019941503\n",
      "\n",
      "matrix 3\n",
      "the norm of the bias vector is 0.05085680963015073\n",
      "value of the bias vector: [-0.02386344 -0.01020399 -0.03682708 -0.02359228]\n",
      "     > cosine similarity with previous right singular vector: 0.999081963709783\n",
      "highest singular vaule: 1.652869120813458\n",
      "second singular vaule: 0.9992543064015142\n",
      "sum of the rest of the singular values : 0.006995868230171003\n",
      "\n",
      "matrix 4\n",
      "the norm of the bias vector is 2.6666481724215565\n",
      "value of the bias vector: [2.66664817]\n",
      "     > cosine similarity with previous right singular vector: -0.5015458932235872\n",
      "highest singular vaule: 3.137077133105031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "product_theta = np.eye(d+1)\n",
    "previous_right_eig = 0\n",
    "\n",
    "for  i in range(len(model.biases)) :\n",
    "    if i+1 < len(model.biases):\n",
    "\n",
    "        slope = model.slopes[i]\n",
    "        bias = model.biases[i]\n",
    "        weight = make_augmented_matrix(slope, bias)\n",
    "    else:\n",
    "        slope = model.slopes[i]\n",
    "        \n",
    "        bias = model.biases[i]\n",
    "        weight = np.zeros((d+1, 1))\n",
    "\n",
    "        weight[0] = bias\n",
    "        weight[1:] = slope\n",
    "\n",
    "    product_theta = product_theta@weight\n",
    "\n",
    "\n",
    "    svd_decomp = np.linalg.svd(weight)\n",
    "\n",
    "\n",
    "    print (f\"matrix {i+1}\")\n",
    "    print(f\"the norm of the bias vector is {norm(bias)}\")\n",
    "    print(f\"value of the bias vector: {bias}\")\n",
    "    # print(f\"highest left singular vectior: {svd_decomp[0].T[0]}\") \n",
    "    if i>0:     \n",
    "        print(f\"     > cosine similarity with previous right singular vector: {previous_right_eig@svd_decomp[0].T[0].T}\")\n",
    "    print(f\"highest singular vaule: {svd_decomp[1][0]}\")\n",
    "    if i ==d-1: \n",
    "        # print(f\"only one singular value\")\n",
    "        pass\n",
    "    \n",
    "    else:  \n",
    "        print(f\"second singular vaule: {svd_decomp[1][1]}\")\n",
    "        print(f\"sum of the rest of the singular values : {np.sum(svd_decomp[1][2:])}\")\n",
    "\n",
    "    # print(f\"highest right singular vector: {svd_decomp[2][0]}\")\n",
    "    previous_right_eig = svd_decomp[2][0]\n",
    "    print()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c21174ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the product of all affine matrices at convergence: [[ 1.18935176 -2.6000652  -6.68353361  1.30049403 -0.65649629]]\n",
      "Minimum least squares solution of the equivalent problem:   [[ 1.18935176 -2.6000652  -6.68353361  1.30049403 -0.65649629]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Value of the product of all affine matrices at convergence: {product_theta.T}\")\n",
    "print(f\"Minimum least squares solution of the equivalent problem:   {theta_star.T}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef32aa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of the Frobenius norm of the sum of all matrices of the corollary of lemma 1: 13.615732198479966\n",
      "value of the Frobenius norm of the sum of all matrices of the corollary of lemma 1: 24.45958619054729\n",
      "value of the Frobenius norm of the sum of all matrices of the corollary of lemma 1: 658837.2450181461\n",
      "NB: These value have been calculated doing the telescopic sum of on the order of 500000 elements, there might be an accumulation of rounding errors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(d-1):\n",
    "    Tele_Sum = 0\n",
    "    for j in range(len(lemma_1_list[i])):\n",
    "        Tele_Sum+=lemma_1_list[i][j]\n",
    "    print(f\"value of the Frobenius norm of the sum of all matrices of the corollary of lemma 1: {np.sum(Tele_Sum**2)}\" )\n",
    "    \n",
    "    \n",
    "print(f\"NB: These value have been calculated doing the telescopic sum of on the order of {nb_steps} elements, there might be an accumulation of rounding errors\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
